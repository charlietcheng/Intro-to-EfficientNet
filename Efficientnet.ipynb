{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M24QrQork-FZ"
      },
      "source": [
        "# CS182 Project: Introduction to EfficientNet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ot2UEHsmDjx"
      },
      "source": [
        "### Part1: introduction\n",
        "EfficientNet is a family of convolutional neural networks that were designed to provide state-of-the-art accuracy on image classification tasks while maintaining a high level of efficiency. Developed by a team of researchers at Google, EfficientNet models use a novel compound scaling method to balance the number of parameters in the network with its depth and width, resulting in a highly optimized architecture that achieves superior performance with fewer computational resources. EfficientNet models have achieved top scores in various computer vision benchmarks, including the ImageNet dataset, and have been widely adopted for a range of applications, including object detection, segmentation, and transfer learning.\n",
        "\n",
        "In this HW, we're going to implement EfficientNet from scratch, and understand how EfficientNets are \"efficient\" in the sense of cost of compuation and number of parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yviRW1EmrXa"
      },
      "source": [
        "Imports and preparations: (Run the cell below if you're using Google Colab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeiRRAQkAulz",
        "outputId": "8ec1899f-e8d4-4bf3-aa40-5a62598b8b3c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "DRIVE_PATH = '/content/gdrive/My\\ Drive/cs182project_eq_efficientnet'\n",
        "DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n",
        "if not os.path.exists(DRIVE_PYTHON_PATH):\n",
        "  %mkdir $DRIVE_PATH\n",
        "\n",
        "## the space in `My Drive` causes some issues,\n",
        "## make a symlink to avoid this\n",
        "SYM_PATH = '/content/cs182project_eq_efficientnet'\n",
        "if not os.path.exists(SYM_PATH):\n",
        "  !ln -s $DRIVE_PATH $SYM_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhaO74GkpGiu",
        "outputId": "931eb82d-9273-493b-f175-9faf364a5f91"
      },
      "outputs": [],
      "source": [
        "!pip install graphviz \n",
        "!apt-get install graphviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50fXbzQ2AyX6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from math import ceil\n",
        "\n",
        "# Torch stuff\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import random_split\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torch.nn.init as init\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "#training part\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import copy\n",
        "\n",
        "import graphviz \n",
        "from itertools import tee"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Elcw_ZZrI-R"
      },
      "outputs": [],
      "source": [
        "#@title Graphviz Utilities (run this)\n",
        "def generate_dwr(csf):\n",
        "  \"\"\"\n",
        "  Determins the depth, width and resolution from the scaling factor.\n",
        "  Alpha, beta, gamma are taken from the efficientnet paper.\n",
        "  \"\"\"\n",
        "  #From the paper\n",
        "  alpha = 1.2\n",
        "  beta = 1.1\n",
        "  gamma = 1.15\n",
        "  return (alpha ** csf, beta ** csf, gamma ** csf)\n",
        "\n",
        "def pairwise(iterable):\n",
        "  \"\"\"\n",
        "  Iterates through an iterable (list), pairwise.\n",
        "  a, b, c -> (a,b), (b,c)\n",
        "  \"\"\"\n",
        "  a, b = tee(iterable)\n",
        "  next(b, None)\n",
        "  return zip(a, b)\n",
        "\n",
        "def compose_edges(g, nodes):\n",
        "  \"\"\"\n",
        "  Forms the actual edges from a list of all the nodes, just sequentially.\n",
        "  \"\"\"\n",
        "  for a, b, in list(pairwise(nodes)):\n",
        "    g.edge(a, b, constraint='false')\n",
        "\n",
        "def num_layers(n):\n",
        "  # This is just a random constant\n",
        "  return max(int(0.8*n), 1)\n",
        "\n",
        "def generate_layers(g, rfactors, color, w_f, h_f, layer_name):\n",
        "  \"\"\"\n",
        "  Generates a colored 'layer' in the graph, this could result in several\n",
        "  Nodes being generated depending on the depth factor.\n",
        "  \"\"\"\n",
        "  d, w, r = rfactors\n",
        "  layers = num_layers(d)\n",
        "  items = []\n",
        "  for layer, index, in enumerate(range(layers)):\n",
        "    name = layer_name + str(index)\n",
        "    g.node(name, label = \" \", color = color, style = \"filled\", width = str(w*w_f), height = str(r*h_f))\n",
        "    items.append(name)\n",
        "  return items\n",
        "\n",
        "def generate_visualization(csf = 1):\n",
        "  factors = generate_dwr(csf)\n",
        "  d, w, r = factors\n",
        "  g = graphviz.Digraph('efficientNet', comment='efficientNet') \n",
        "  all_items = []\n",
        "  g.attr('node', shape='box')\n",
        "  g.node('input','input' ,color = '#ffffff')\n",
        "  all_items.append('input')\n",
        "  all_items += generate_layers(g, factors, color = '#b873bf', w_f = 0.1, h_f = 0.3, layer_name = 'first')\n",
        "  all_items += generate_layers(g, factors, color = '#e3c062', w_f = 0.2, h_f = 0.2, layer_name = 'second')\n",
        "  all_items += generate_layers(g, factors, color = '#62e3a2', w_f = 0.2, h_f = 0.4, layer_name = 'third')\n",
        "  all_items += generate_layers(g, factors, color = '#62dfe3', w_f = 0.02, h_f = 0.6, layer_name = 'fourth')\n",
        "  all_items += generate_layers(g, factors, color = '#e362d0', w_f = 0.02, h_f = 0.8, layer_name = 'fifth')\n",
        "  g.node('output', 'output', color = '#ffffff')\n",
        "  all_items.append('output')\n",
        "  compose_edges(g, all_items)\n",
        "  g.attr(label=r'EfficientNet Architecture Diagram \\n Compound Scaling Factor = %s' %(csf))\n",
        "  return g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQHzG5LhrYUF"
      },
      "source": [
        "To better reinforce the intution about the compound scaling method, we have implemented a visualization generator function. The overall intution about just changing the compound scaling factor, then being able to affect overall change in the actual architecture, changing the depth, width, and resolution in a principled manner.\n",
        "\n",
        "* This is intended to provide intuition about efficientNet, this is not how efficientNet literally scales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc1plM7ZrcSP"
      },
      "source": [
        "Question: Play around with this function, at what point do you start to see new depth layers emerging? (You might also want to use this opportunity to check your intuition about the previous conceptual questions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "CbBlOojKsMD4",
        "outputId": "a1a708d5-37cc-475f-a9c3-f4d0a3df2cf6"
      },
      "outputs": [],
      "source": [
        "generate_visualization(csf = 2) #Play around with this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhibDcKWm88v"
      },
      "source": [
        "We're going to use CIFAR-10 dataset for this project. It is very commonly used while testing certain CV models. The dataset contains 60,000 32x32 color images in 10 different classes. The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. There are 6,000 images of each class. We're going to use the torchvision package to load the dataset. The torchvision package consists of popular datasets, model architectures, and common image transformations for computer vision.\n",
        "\n",
        "In this part we're going to implement a dataloader. The purpose of this is to build a convenient way to feed data from a dataset to a model during training or inference. With the DataLoader, users can easily handle large datasets and apply different data augmentation techniques to the input data. The PyTorch DataLoader is a flexible and efficient tool that has become a standard part of many deep learning workflows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEiiw_KIBFbF"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        \n",
        "    ]\n",
        ")\n",
        "\n",
        "test_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(), \n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64mqlIB-Wa0J",
        "outputId": "caec61e1-2ee2-4823-a81f-73c89491df8b"
      },
      "outputs": [],
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=train_transform)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                        download=True, transform=test_transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHv1wFh8OBb-",
        "outputId": "edfd7eec-be7c-4af0-f848-6bdd1bb65747"
      },
      "outputs": [],
      "source": [
        "train_size, val_size = 40000, 10000\n",
        "train_ds, val_ds = random_split(trainset, [train_size, val_size])\n",
        "len(train_ds), len(val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4kM7X8q-AF7"
      },
      "source": [
        "Question 1a): setup train_loader, train_dataset, val_loader, val_dataset, test_loader, test_dataset in the following block. You can use the code in the previous block as a reference.\n",
        "Hint: check out function: produce_dataloader_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "467s3tpEBJtv"
      },
      "outputs": [],
      "source": [
        "#######################\n",
        "# TODO: your code here#\n",
        "#######################\n",
        "trainloader = ...\n",
        "valloader = ...\n",
        "testloader = ...\n",
        "#######################\n",
        "#END OF YOUR CODE    #\n",
        "#######################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "KsgBNtvxpGix",
        "outputId": "dc15683a-99b5-4620-e1bc-5b1dd9bf36a8"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "You should be able to see some sample images from the training set if you correctly implemented the above code.\n",
        "'''\n",
        "def draw_sample_images(data, labels):\n",
        "    nrows = 4\n",
        "    ncols = 10\n",
        "    total_image = data.shape[0]\n",
        "    samples = np.random.choice(total_image, nrows*ncols)\n",
        "    plt.figure(figsize=(20, 5))\n",
        "    for i in range(nrows*ncols):\n",
        "        plt.subplot(nrows, ncols, i+1)\n",
        "        image = np.moveaxis(data[samples[i]].numpy(), 0, -1)\n",
        "        plt.imshow(image/2+0.5)\n",
        "        plt.title(trainset.classes[labels[samples[i]]])\n",
        "        plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "data_iterator = iter(trainloader)\n",
        "images, labels = next(data_iterator)\n",
        "draw_sample_images(images, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAG0AT-qPmjA",
        "outputId": "51daf4b6-3f38-4229-b13b-0f20433a844f"
      },
      "outputs": [],
      "source": [
        "classes = trainset.classes\n",
        "classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aUNsVP1-AF8"
      },
      "source": [
        "Question: What's the shape of data in train_loader for a sigle batch? (in terms of [N, C, H, W])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dEH-wldsN_0"
      },
      "source": [
        "### Part 2: Building the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a12qpdF5sq-W"
      },
      "source": [
        "The \"highlight\" of EfficientNet is its use of compound scaling methods. Compound scaling in essence is to use a coefficient to uniformaly scale the 3 Dimensions (depth, width and resolution) of the model. The coefficient is denoted as $\\phi$ in the paper. The scaling method is as follows:\n",
        "\n",
        "$$\n",
        " \\text{depth}: d = \\alpha^\\phi \\\\\n",
        " \\text{width}: w = \\beta^\\phi \\\\\n",
        " \\text{resolution}: r = \\gamma^\\phi \\\\\n",
        " \\alpha \\cdot \\beta^2 \\cdot \\gamma ^2 \\approx 2\\\\\n",
        "\\alpha \\geq 1, \\beta \\geq 1, \\gamma \\geq 1\n",
        "$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-R0zsi4-AF8"
      },
      "source": [
        "The author created a family of EfficientNet models with different $\\phi$ values, and the largest model is EfficientNet-B7 with $\\phi = 2.0$. In this HW, we're going to implement EfficientNet, with the ability to scale from $b0$ to $b7$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOFTJtgf-AF8"
      },
      "source": [
        "Firstly we'going to implement some tricks the author used that makes EfficientNet efficient. The first technique is called Squeeze and Excitation (SE). SE is very similiar to the attention mechanism. It is used to help the model to focus on the most important features. The SE module is implemented as follows: \n",
        "\n",
        "![image](https://github.com/charlietcheng/Intro-to-EfficientNet/blob/main/imgs/SE.jpeg?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbQ87NkFBfpo"
      },
      "outputs": [],
      "source": [
        "# Implement the Squueze and Excitation block down below\n",
        "# Note that though the image above shows we're using ReLu as the activation function, we're actually using SiLU (swish)\n",
        "# as the author mentioned in the paper that it performed better than ReLU. We also replace the avgpool with adaptiveavgpool\n",
        "# for the same reason.\n",
        "# Hint: use nn.AdaptiveAvgPool2d(1) to replace nn.AvgPool2d(1) and nn.SiLU() to replace nn.ReLU()\n",
        "class SqueezeExcitation(nn.Module):\n",
        "    def __init__(self, n_in, reduced_dim, fixed_params=False):\n",
        "        super(SqueezeExcitation, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(n_in, reduced_dim, kernel_size=1)\n",
        "        self.conv2 = nn.Conv2d(reduced_dim, n_in, kernel_size=1)\n",
        "        if fixed_params:\n",
        "          init.constant_(self.conv1.weight, 0.01)\n",
        "          init.constant_(self.conv1.bias, 0.0)\n",
        "          init.constant_(self.conv2.weight, 0.01)\n",
        "          init.constant_(self.conv2.bias, 0.0)\n",
        "        '''\n",
        "        Hints: follow the pipeline in the image above to implement the forward pass of the Squeeze and Excitation block.\n",
        "        use nn.Sequential() to build the block.\n",
        "        '''\n",
        "        #######################\n",
        "        # TODO: your code here#\n",
        "        #######################\n",
        "        self.se = ...\n",
        "        #######################\n",
        "        # End of your code    #\n",
        "        #######################\n",
        "    \n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Hints: one line of code\n",
        "        '''\n",
        "        #######################\n",
        "        # TODO: your code here#\n",
        "        #######################\n",
        "        raise NotImplementedError(\"Squeeze and Excitation forward pass not implemented\")\n",
        "        #######################\n",
        "        # End of your code    #\n",
        "        #######################\n",
        "        # Hint: consider why what the picture means by scaling and why we're multiplying here\n",
        "        return x * y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YHcJK2HpGiz",
        "outputId": "049a520f-1a9b-4ace-9067-2fc3bab48822"
      },
      "outputs": [],
      "source": [
        "# unit test for se\n",
        "def test_se():\n",
        "  data_iterator = iter(testloader)\n",
        "  images, labels = next(data_iterator)\n",
        "  x = images[0]\n",
        "  se = SqueezeExcitation(3,2,fixed_params = True)\n",
        "  y = se(x)\n",
        "  y_test = y[0][0][:5].detach()\n",
        "  y_true = torch.tensor([0.1196, 0.1235, 0.1471, 0.1510, 0.1274])\n",
        "  assert torch.allclose(y_test,y_true,atol = 1e-3)\n",
        "test_se()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdUhAsTL-AF8"
      },
      "source": [
        "Next we're going to implement the trick: Stochastic Depth, which makes the entire training process much faster. The gist of it is to randomly drop a subset of layers and bypass them with the identity function during training. And a full network is used during testing/inference. \n",
        "\n",
        "The image below shows the implementation of Stochastic Depth, we suggest lo\n",
        "\n",
        "![image](https://github.com/charlietcheng/Intro-to-EfficientNet/blob/main/imgs/SD.jpeg?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kU5a8MBO-AF8"
      },
      "outputs": [],
      "source": [
        "# Implement the Stochastic Depth block down below\n",
        "# Hint: use torch.rand() to generate a random number\n",
        "\n",
        "class StochasticDepth(nn.Module):\n",
        "    \n",
        "    def __init__(self, survival_prob = 0.8, fixed_params=False):\n",
        "        super(StochasticDepth, self).__init__()\n",
        "        self.fixed_params = fixed_params\n",
        "        self.p =  survival_prob\n",
        "        \n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Hints: what happens when self.training is True? What shall we do when self.training is False?\n",
        "        The idea is kind of similiar to Dropout and Masking.\n",
        "        '''\n",
        "        if self.fixed_params:\n",
        "          if torch.cuda.is_available():\n",
        "              torch.cuda.manual_seed(10)\n",
        "          else:\n",
        "              torch.manual_seed(10)\n",
        "        #######################\n",
        "        # TODO: your code here#\n",
        "        #######################\n",
        "        raise NotImplementedError(\"Stochastic Depth forward pass not implemented\")\n",
        "        #######################\n",
        "        # End of your code    #\n",
        "        #######################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UdegJfDpGiz"
      },
      "outputs": [],
      "source": [
        "# unit test for stochastic depth\n",
        "def test_sd():\n",
        "  data_iterator = iter(testloader)\n",
        "  images, labels = next(data_iterator)\n",
        "  x = images[0]\n",
        "  sd = StochasticDepth(fixed_params = True)\n",
        "  y = sd(x)\n",
        "  y_test = y[0][0][0][:5].detach()\n",
        "  y_true = torch.tensor([0.2990, 0.3088, 0.3676, 0.3775, 0.3186])\n",
        "  assert torch.allclose(y_test,y_true,atol = 1e-3)\n",
        "test_sd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVmgidXv-AF8"
      },
      "outputs": [],
      "source": [
        "# Here we provide you with the simple Conv-BatchNorm-Activation block for you\n",
        "# Note that we're using SiLU (swish) as the activation function instead of ReLU as the author mentioned in the paper \n",
        "# that it performed better than ReLU\n",
        "\n",
        "class ConvBnAct(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_in, n_out, kernel_size = 3, stride = 1, \n",
        "                 padding = 0, groups = 1, bn = True, act = True,\n",
        "                 bias = False, fixed_params = False\n",
        "                ):\n",
        "        \n",
        "        super(ConvBnAct, self).__init__()\n",
        "        self.conv = nn.Conv2d(n_in, n_out, kernel_size = kernel_size,\n",
        "                              stride = stride, padding = padding,\n",
        "                              groups = groups, bias = bias\n",
        "                             )\n",
        "        if fixed_params:\n",
        "          init.constant_(self.conv.weight, 0.01)\n",
        "        if bias:\n",
        "          init.constant_(self.conv.bias, 0.0)\n",
        "          \n",
        "        self.batch_norm = nn.BatchNorm2d(n_out) if bn else nn.Identity()\n",
        "        self.activation = nn.SiLU() if act else nn.Identity()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.conv(x)\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cmt_NFdb-AF8"
      },
      "source": [
        "Finally here come the finally implementation of EfficientNet. Some additional tricks the author used here include Depthwise Separable Convolution, which is a combination of depthwise convolution and pointwise convolution. The depthwise convolution is used to extract features from each channel, and the pointwise convolution is used to combine the features from different channels.\n",
        "\n",
        "The image below shows the implementation of Depthwise Separable Convolution, we suggest referencing this image when performing the implementation:\n",
        "\n",
        "![image](https://github.com/charlietcheng/Intro-to-EfficientNet/blob/main/imgs/DSC.jpeg?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zd3KN8Rj-AF8"
      },
      "outputs": [],
      "source": [
        "# We start by implementing Residual Bottleneck Block with Expansion Factor = N as defined in Mobilenet-V2 paper\n",
        "# with Squeeze and Excitation Block and Stochastic Depth. \n",
        "# The process of implementation: residual -> exapnd -> depthwise conv \n",
        "# -> squeeze and excitation -> pointwise conv -> skip connection\n",
        "\n",
        "class MBConvN(nn.Module):\n",
        "    def __init__(self, n_in, n_out, kernel_size = 3, \n",
        "                 stride = 1, expansion_factor = 6,\n",
        "                 reduction = 4, # Squeeze and Excitation Block\n",
        "                 survival_prob = 0.8, # Stochastic Depth\n",
        "                 fixed_params = False\n",
        "                ):\n",
        "        \n",
        "        super(MBConvN, self).__init__()\n",
        "        '''\n",
        "        Hints: self.skip_connection is True if stride == 1 and n_in == n_out\n",
        "\n",
        "        For self.expand, you can use nn.Identity() if expansion_factor == 1 else ConvBnAct(). Check the original paper \n",
        "        for more details for the parameter you are going to use.\n",
        "\n",
        "        For self.depthwise_conv, you can use ConvBnAct() with the correct parameters.\n",
        "\n",
        "        self.se is something you implemented above.\n",
        "\n",
        "        self.pointwise_conv is similar to self.depthwise_conv, but with different parameters.\n",
        "\n",
        "        self.drop_layers is something you implemented above as well.\n",
        "        '''\n",
        "        #######################\n",
        "        # TODO: your code here#\n",
        "        #######################\n",
        "        self.skip_connection = ...       \n",
        "        self.expand = ...\n",
        "        self.depthwise_conv = ...\n",
        "        self.se = ...\n",
        "        self.pointwise_conv = ...\n",
        "        self.drop_layers = ...\n",
        "        #######################\n",
        "        # End of your code    #\n",
        "        #######################\n",
        "        \n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        '''\n",
        "        Hints: if self.skip_connection is True, you should add residual to x before returning it.\n",
        "        Recollect the pipline of the MBConvN: residual -> exapnd -> depthwise conv -> squeeze and excitation \n",
        "        -> pointwise conv -> skip connection\n",
        "        '''\n",
        "        #######################\n",
        "        # TODO: your code here#\n",
        "        #######################\n",
        "        x = ...\n",
        "        #######################\n",
        "        # End of your code    #\n",
        "        #######################\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f80mmCDV-AF8"
      },
      "outputs": [],
      "source": [
        "# Here comes the acutal implmentation of EfficientNet\n",
        "class EfficientNet(nn.Module):\n",
        "    def __init__(self, width_mult = 1, depth_mult = 1, \n",
        "                dropout_rate = 0.2, num_classes = 1000, seed=42, fixed_params = False):\n",
        "        super(EfficientNet, self).__init__()\n",
        "        last_channel = ceil(1280 * width_mult)\n",
        "        self.features = self._feature_extractor(width_mult, depth_mult, last_channel, fixed_params)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = nn.Linear(last_channel, num_classes)\n",
        "        if fixed_params:\n",
        "          init.constant_(self.fc1.weight, 0.01)\n",
        "          init.constant_(self.fc1.bias, 0.0)\n",
        "          self.classifier = nn.Sequential(\n",
        "              self.fc1\n",
        "          )\n",
        "        else:\n",
        "          self.classifier = nn.Sequential(\n",
        "              nn.Dropout(dropout_rate),\n",
        "              self.fc1\n",
        "          )\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.classifier(x.view(x.shape[0], -1))\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def _feature_extractor(self, width_mult, depth_mult, last_channel, fixed_params):\n",
        "        \n",
        "        channels = 4*ceil(int(32*width_mult) / 4)\n",
        "        layers = [ConvBnAct(3, channels, kernel_size = 3, stride = 2, padding = 1, fixed_params=fixed_params)]\n",
        "        in_channels = channels\n",
        "        \n",
        "        # These are from the paper\n",
        "        kernels = [3, 3, 5, 3, 5, 5, 3]\n",
        "        expansions = [1, 6, 6, 6, 6, 6, 6]\n",
        "        num_channels = [16, 24, 40, 80, 112, 192, 320]\n",
        "        num_layers = [1, 2, 2, 3, 3, 4, 1]\n",
        "        strides =[1, 2, 2, 2, 1, 2, 1]\n",
        "        \n",
        "        # Scale channels and num_layers according to width and depth multipliers.\n",
        "        scaled_num_channels = [4*ceil(int(c*width_mult) / 4) for c in num_channels]\n",
        "        scaled_num_layers = [int(d * depth_mult) for d in num_layers]\n",
        "\n",
        "        '''\n",
        "        Hints: save all layers in the list `layers` and we will use nn.Sequential to wrap them up.\n",
        "        You first use a for loop to iterate through all scaled number of layers. Then, for each iteration,\n",
        "        you use another for loop to iterate through all scaled number of channels. For each iteration, you\n",
        "        append a MBConvN block to the list `layers`. Note that the first MBConvN block in each iteration\n",
        "        should have a stride of `strides[i]` and the rest should have a stride of 1. Also, the first MBConvN\n",
        "        block in each iteration should have an input channel of `in_channels` and the rest should have an input\n",
        "        channel of `scaled_num_channels[i]`. After each iteration, you update `in_channels` to be\n",
        "        `scaled_num_channels[i]`.\n",
        "        '''\n",
        "        #######################\n",
        "        # TODO: your code here#\n",
        "        #######################\n",
        "        layers = []\n",
        "        #######################\n",
        "        # End of your code    #\n",
        "        #######################\n",
        "        \n",
        "        layers.append(ConvBnAct(in_channels, last_channel, kernel_size = 1, stride = 1, padding = 0, fixed_params = fixed_params))\n",
        "    \n",
        "        return nn.Sequential(*layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdS-qNxZpGi1"
      },
      "outputs": [],
      "source": [
        "# unit test for efficientnet\n",
        "def test_efficientnet():\n",
        "  data_iterator = iter(testloader)\n",
        "  images, labels = next(data_iterator)\n",
        "  x = images[:2]\n",
        "  net = EfficientNet(fixed_params=True)\n",
        "  y = net(x)\n",
        "  y_test = y[:,0].detach()\n",
        "  y_true = torch.tensor([-3.4425,  9.3575])\n",
        "  assert torch.allclose(y_test,y_true,atol = 1e-3)\n",
        "\n",
        "test_efficientnet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L54OyLZn-AF9"
      },
      "outputs": [],
      "source": [
        "# Compound scaling factors for efficient-net family.\n",
        "efficient_net_config = {\n",
        "    # tuple of width multiplier, depth multiplier, resolution, and Survival Prob\n",
        "    # from the paper\n",
        "    \"b0\" : (1.0, 1.0, 224, 0.2),\n",
        "    \"b1\" : (1.0, 1.1, 240, 0.2),\n",
        "    \"b2\" : (1.1, 1.2, 260, 0.3),\n",
        "    \"b3\" : (1.2, 1.4, 300, 0.3),\n",
        "    \"b4\" : (1.4, 1.8, 380, 0.4),\n",
        "    \"b5\" : (1.6, 2.2, 456, 0.4),\n",
        "    \"b6\" : (1.8, 2.6, 528, 0.5),\n",
        "    \"b7\" : (2.0, 3.1, 600, 0.5)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdPUGUqm-AF9"
      },
      "source": [
        "Finally we're going to train our implemented model. Follow the code instruction below to train the model. We recommend to use GPU to train the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNs7Yi15-AF9"
      },
      "outputs": [],
      "source": [
        "def calculate_loss_and_accuracy(model, dataloader, size_of_dataset, criterion):\n",
        "    \n",
        "    # Now set model to validation mode.\n",
        "    running_loss = 0\n",
        "    running_accuracy = 0\n",
        "    \n",
        "    # Processing the Test Loader\n",
        "    for (inputs, labels) in dataloader:\n",
        "        \n",
        "        # Load data to device.\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Outputs\n",
        "        outputs = model(inputs)\n",
        "        _ , preds = torch.max(outputs, 1)\n",
        "        \n",
        "        # Outputs\n",
        "        outputs = model(inputs)\n",
        "        _ , preds = torch.max(outputs, 1)\n",
        "        \n",
        "        # Loss and Backpropagation.\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Statistics\n",
        "        running_loss += loss.item()*inputs.size(0)\n",
        "        running_accuracy += torch.sum(preds == labels.data)\n",
        "        \n",
        "    epoch_loss = running_loss/size_of_dataset\n",
        "    epoch_accuracy = running_accuracy/size_of_dataset\n",
        "    \n",
        "    return epoch_loss, epoch_accuracy\n",
        "\n",
        "def train(model, criterion, optimizer, scheduler, num_of_epochs):\n",
        "    \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    track_training_loss = []\n",
        "    track_val_loss = []\n",
        "    track_val_acc = []\n",
        "\n",
        "    for epoch in range(num_of_epochs):\n",
        "\n",
        "        print(f'\\nEpoch {epoch + 1}/{num_of_epochs}')\n",
        "        print('-'*30)\n",
        "\n",
        "        model.train() # Setting model to train.\n",
        "        running_loss = 0\n",
        "        running_accuracy = 0\n",
        "\n",
        "        # Processing the Train Loader\n",
        "        for (inputs, labels) in trainloader:\n",
        "\n",
        "            '''\n",
        "            Load data to device.\n",
        "            Hints: use .to(device) to load data to device\n",
        "            remember to zero the parameter gradients\n",
        "            '''\n",
        "            #######################\n",
        "            # TODO: your code here#\n",
        "            #######################\n",
        "            raise NotImplementedError(\"Training not implemented\")\n",
        "            #######################\n",
        "            # End of your code    #\n",
        "            #######################\n",
        "\n",
        "            # Outputs\n",
        "            outputs = model(inputs)\n",
        "            _ , preds = torch.max(outputs, 1)\n",
        "\n",
        "            '''\n",
        "            Loss and Backpropagation.\n",
        "            Hints: use criterion to calculate loss\n",
        "            remember to perform backpropagation\n",
        "            '''\n",
        "            #######################\n",
        "            # TODO: your code here#\n",
        "            #######################\n",
        "            raise NotImplementedError(\"Training not implemented\")\n",
        "            #######################\n",
        "            # End of your code    #\n",
        "            #######################\n",
        "            # Statistics\n",
        "            running_loss += loss.item()*inputs.size(0)\n",
        "            running_accuracy += torch.sum(preds == labels.data)\n",
        "        \n",
        "        scheduler.step()\n",
        "        epoch_loss = running_loss/len(trainset)\n",
        "        epoch_accuracy = running_accuracy/len(trainset)\n",
        "        track_training_loss.append(epoch_loss) # Loss Tracking\n",
        "\n",
        "        print(f'Training Loss: {epoch_loss:.4f} Training Acc.: {epoch_accuracy:.4f}')\n",
        "\n",
        "        # Now set model to validation mode.\n",
        "        model.eval()\n",
        "\n",
        "        val_loss, val_accuracy = calculate_loss_and_accuracy(model, valloader, len(val_ds), criterion)\n",
        "        track_val_loss.append(val_loss) \n",
        "        track_val_acc.append(val_accuracy)\n",
        "        if val_accuracy > best_acc:\n",
        "            print(\"Found better model...\")\n",
        "            print('Updating the model weights....\\n')\n",
        "            print(f'Val Loss: {val_loss:.4f} Val Acc.: {val_accuracy:.4f}\\n')\n",
        "\n",
        "            best_acc = val_accuracy\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "     \n",
        "    model.load_state_dict(best_model_wts) # update model\n",
        "    \n",
        "    return  model, track_val_loss, track_val_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gm6WUbht-AF9",
        "outputId": "ae5417b9-aba9-425d-d6ba-a87abe5f6955"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda')\n",
        "\n",
        "NUM_OF_CLASSES = 10\n",
        "BATCH_SIZE = 32\n",
        "NUM_OF_EPOCHS = 30\n",
        "\n",
        "# Initialize Efficientnet model\n",
        "# We are training the b2 version here\n",
        "version = 'b2'\n",
        "width_mult, depth_mult, res, dropout_rate = efficient_net_config[version]\n",
        "model = EfficientNet(width_mult, depth_mult, dropout_rate, num_classes = NUM_OF_CLASSES)\n",
        "model = model.to(device) # Load model to device.\n",
        "\n",
        "# Criterion.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-6, weight_decay=1e-2)\n",
        "\n",
        "\n",
        "exp_lr_scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-2,\n",
        "                                             steps_per_epoch=len(trainloader), epochs=NUM_OF_EPOCHS)\n",
        "\n",
        "# Training\n",
        "best_model = train(model = model,\n",
        "                   criterion = criterion,\n",
        "                   optimizer = optimizer,\n",
        "                   scheduler = exp_lr_scheduler,\n",
        "                   num_of_epochs = NUM_OF_EPOCHS\n",
        "                  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE24zpzztRkP"
      },
      "source": [
        "Question: What is the best accuracy you can get? What is the best accuracy you can get with the same number of parameters as the EfficientNet-B2 model? Feel free to use different models and find the one with the best validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOjej3zOtAan"
      },
      "outputs": [],
      "source": [
        "def train_model(model):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  optimizer = optim.AdamW(model.parameters(), lr=1e-6, weight_decay=1e-2)\n",
        "\n",
        "\n",
        "  exp_lr_scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-2,\n",
        "                                              steps_per_epoch=len(trainloader), epochs=NUM_OF_EPOCHS)\n",
        "\n",
        "  best_model, val_losses, val_accs = train(model = model,\n",
        "                    criterion = criterion,\n",
        "                    optimizer = optimizer,\n",
        "                    scheduler = exp_lr_scheduler,\n",
        "                    num_of_epochs = NUM_OF_EPOCHS\n",
        "                    )\n",
        "  \n",
        "  val_loss, val_accuracy = calculate_loss_and_accuracy(best_model, valloader, len(val_ds), criterion)\n",
        "  print(\"final validation statistics, loss : %s, accuracy : %s\" %(val_loss, val_accuracy))\n",
        "\n",
        "  return best_model, val_losses, val_accs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap8IP4-ntWPW"
      },
      "source": [
        "Now, let us compare with some other recent architecture models, which torchvision conveniently packages. Generally, we want to consider 2 things, the number of parameters, and the actual performance of the architecture.\n",
        "\n",
        "For this, let us consider densenet121, mobilenetv2 and resNet50, which are all fairly recent models.\n",
        "\n",
        "**Warning: this part takes roughly half an hour to train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXTwBnd9tYOg"
      },
      "outputs": [],
      "source": [
        "densenet121 = torchvision.models.densenet121(weights = False).to(device)\n",
        "mobilenetv2 = torchvision.models.mobilenet_v2(weights = False).to(device)\n",
        "resNet50 = torchvision.models.resnet50(weights = False).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5HXLxqj1xzu"
      },
      "outputs": [],
      "source": [
        "def calculate_parameters(model, model_name):\n",
        "  num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  print(\"%s has %s params\" %(model_name, num_params))\n",
        "  return num_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDRODdnrtYRw"
      },
      "outputs": [],
      "source": [
        "dense_params = train_model(densenet121)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUdTM59Pteih"
      },
      "outputs": [],
      "source": [
        "mobile_params = train_model(mobilenetv2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxZq2N-OtbUb"
      },
      "outputs": [],
      "source": [
        "res_params = train_model(resNet50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJSoFkRZtbb-"
      },
      "outputs": [],
      "source": [
        "den_model, den_losses, den_acc = dense_params\n",
        "mobile_model, mobile_losses, mobile_acc = mobile_params\n",
        "res_model, res_losses, res_acc = res_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WA-F120rDIMM"
      },
      "outputs": [],
      "source": [
        "den_acc = np.array([i.to('cpu').numpy() for i in den_acc])\n",
        "mobile_acc = np.array([i.to('cpu').numpy() for i in mobile_acc])\n",
        "res_acc = np.array([i.to('cpu').numpy() for i in res_acc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eq3y5q3DUPd"
      },
      "outputs": [],
      "source": [
        "best_model, efficientnet_val_losses, efficientnet_val_accs = best_model\n",
        "efficientnet_val_accs = np.array([i.to('cpu').numpy() for i in efficientnet_val_accs])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5QBGztvtk1s"
      },
      "source": [
        "Let us see how EfficientNet performs against them!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRxvIpMOthyI"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(NUM_OF_EPOCHS), efficientnet_val_accs, label = 'efficientnetb2')\n",
        "plt.plot(range(NUM_OF_EPOCHS), den_acc, label = 'densenet121')\n",
        "plt.plot(range(NUM_OF_EPOCHS), mobile_acc, label = 'mobilenetv2')\n",
        "plt.plot(range(NUM_OF_EPOCHS), res_acc, label = 'resNet50')\n",
        "plt.title(\"Validation Accuracy vs. Epochs on CIFAR, Models\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlkkJVD9tqYA"
      },
      "source": [
        "Question: Looking at the accuracy of EfficientNet compared to other state-of-the-art model architectures, how does the validation accuracy compare? Anything else interesting you've noticed about these plots?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0QEeemrtqaY"
      },
      "source": [
        "We now examine the number of parameters within the different models. We provide a calculate_parameters function to just sum all the parameters for a model. We also provide some code for a quick bar plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekndvASEth0e"
      },
      "outputs": [],
      "source": [
        "def calculate_parameters(model, model_name):\n",
        "  num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  print(\"%s has %s params\" %(model_name, num_params))\n",
        "  return num_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIWAElestuzy"
      },
      "outputs": [],
      "source": [
        "eff_num_params = calculate_parameters(best_model, \"efficientnetb2\")\n",
        "den_num_params = calculate_parameters(den_model, \"densenet121\")\n",
        "mob_num_params = calculate_parameters(mobile_model, \"mobilenetv2\")\n",
        "res_num_params = calculate_parameters(res_model, \"resNet50\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F90YmY77tu7O"
      },
      "outputs": [],
      "source": [
        "plt.bar(['efficientnetb2', 'densenet121', 'mobilenetv2', 'resNet50'], [eff_num_params, den_num_params, mob_num_params, res_num_params])\n",
        "plt.legend()\n",
        "plt.title(\"Number of Params vs. Model\")\n",
        "plt.xlabel(\"Model\")\n",
        "plt.ylabel(\"Number of Parameters\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te7-HgPqtzX9"
      },
      "source": [
        "Question: Examine the number of parameters of the different models, does anything stand out to you? If we take this as context, is there anything you want to comment about the validation performance of the models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWdqxphgtzaf"
      },
      "source": [
        "This is the end of the notebook. We hope you've learned something about EfficientNet!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Kalf4TW-AF9"
      },
      "source": [
        "## References\n",
        "\n",
        "[1] Mingxing Tan, Quoc V. Le. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. https://arxiv.org/abs/1905.11946"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
